{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Set the path to your dataset directory\n",
    "dataset_dir = \"dataset_2_final\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 423 images belonging to 7 classes.\n",
      "Found 102 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Set image and batch size\n",
    "image_size = (300, 300)\n",
    "batch_size = 4\n",
    "\n",
    "# Create the ImageDataGenerator with data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2  # Splitting the dataset into training and validation sets\n",
    ")\n",
    "\n",
    "# Load the training dataset using image_dataset_from_directory\n",
    "train_ds = train_datagen.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse',  # For integer labels\n",
    "    shuffle=True,\n",
    "    subset='training',    # Selecting the training set\n",
    "    seed=1337\n",
    ")\n",
    "\n",
    "# Load the validation dataset using image_dataset_from_directory\n",
    "val_ds = train_datagen.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse',  # For integer labels\n",
    "    shuffle=False,        # No need to shuffle the validation set\n",
    "    subset='validation',  # Selecting the validation set\n",
    "    seed=1337\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# Check the number of classes in your dataset\n",
    "num_classes = len(train_ds.class_indices)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EfficientNetB0: Image input size is (224, 224, 3)\n",
    "EfficientNetB1: Image input size is (240, 240, 3)\n",
    "EfficientNetB2: Image input size is (260, 260, 3)\n",
    "EfficientNetB3: Image input size is (300, 300, 3)\n",
    "EfficientNetB4: Image input size is (380, 380, 3)\n",
    "EfficientNetB5: Image input size is (456, 456, 3)\n",
    "EfficientNetB6: Image input size is (528, 528, 3)\n",
    "EfficientNetB7: Image input size is (600, 600, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the EfficientNetB3 model\n",
    "base_model = EfficientNetB3(weights='imagenet', include_top=False, input_shape=image_size + (3,))\n",
    "x = base_model.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "predictions = layers.Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "106/106 [==============================] - 114s 765ms/step - loss: 1.8764 - accuracy: 0.2931 - val_loss: 2.5003 - val_accuracy: 0.1961\n",
      "Epoch 2/20\n",
      "106/106 [==============================] - ETA: 0s - loss: 1.5946 - accuracy: 0.4303"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\diosdadj\\OneDrive - HP Inc\\Master\\Lung Cancer Project\\lung_tissues_classification\\generator.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diosdadj/OneDrive%20-%20HP%20Inc/Master/Lung%20Cancer%20Project/lung_tissues_classification/generator.ipynb#X36sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m steps_per_epoch \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_ds)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diosdadj/OneDrive%20-%20HP%20Inc/Master/Lung%20Cancer%20Project/lung_tissues_classification/generator.ipynb#X36sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m validation_steps \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(val_ds)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/diosdadj/OneDrive%20-%20HP%20Inc/Master/Lung%20Cancer%20Project/lung_tissues_classification/generator.ipynb#X36sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diosdadj/OneDrive%20-%20HP%20Inc/Master/Lung%20Cancer%20Project/lung_tissues_classification/generator.ipynb#X36sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     train_ds,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diosdadj/OneDrive%20-%20HP%20Inc/Master/Lung%20Cancer%20Project/lung_tissues_classification/generator.ipynb#X36sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mval_ds,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diosdadj/OneDrive%20-%20HP%20Inc/Master/Lung%20Cancer%20Project/lung_tissues_classification/generator.ipynb#X36sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diosdadj/OneDrive%20-%20HP%20Inc/Master/Lung%20Cancer%20Project/lung_tissues_classification/generator.ipynb#X36sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diosdadj/OneDrive%20-%20HP%20Inc/Master/Lung%20Cancer%20Project/lung_tissues_classification/generator.ipynb#X36sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diosdadj/OneDrive%20-%20HP%20Inc/Master/Lung%20Cancer%20Project/lung_tissues_classification/generator.ipynb#X36sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py:1215\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1201\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_eval_data_handler\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1202\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eval_data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39mget_data_handler(\n\u001b[0;32m   1203\u001b[0m       x\u001b[39m=\u001b[39mval_x,\n\u001b[0;32m   1204\u001b[0m       y\u001b[39m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1213\u001b[0m       model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[0;32m   1214\u001b[0m       steps_per_execution\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution)\n\u001b[1;32m-> 1215\u001b[0m val_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(\n\u001b[0;32m   1216\u001b[0m     x\u001b[39m=\u001b[39;49mval_x,\n\u001b[0;32m   1217\u001b[0m     y\u001b[39m=\u001b[39;49mval_y,\n\u001b[0;32m   1218\u001b[0m     sample_weight\u001b[39m=\u001b[39;49mval_sample_weight,\n\u001b[0;32m   1219\u001b[0m     batch_size\u001b[39m=\u001b[39;49mvalidation_batch_size \u001b[39mor\u001b[39;49;00m batch_size,\n\u001b[0;32m   1220\u001b[0m     steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[0;32m   1221\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1222\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1223\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1224\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1225\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1226\u001b[0m     _use_cached_eval_dataset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   1227\u001b[0m val_logs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m   1228\u001b[0m epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py:1501\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m, step_num\u001b[39m=\u001b[39mstep, _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1500\u001b[0m   callbacks\u001b[39m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 1501\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_function(iterator)\n\u001b[0;32m   1502\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1503\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    882\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 885\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    887\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    888\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:924\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    921\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    922\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    923\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 924\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    925\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    926\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    927\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3036\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   3037\u001b[0m   (graph_function,\n\u001b[0;32m   3038\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   3040\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1960\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1961\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1962\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1963\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1964\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1965\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m     args,\n\u001b[0;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1968\u001b[0m     executing_eagerly)\n\u001b[0;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    592\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    593\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    594\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    595\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    596\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    597\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    598\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    599\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    600\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    604\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     60\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 20\n",
    "steps_per_epoch = len(train_ds)\n",
    "validation_steps = len(val_ds)\n",
    "\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the ImageDataGenerator with data augmentation and cropping\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2  # Splitting the dataset into training and validation sets\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom data generator to generate four cropped images per input image\n",
    "\n",
    "def custom_data_generator(data_generator, steps_per_epoch):\n",
    "    for i, (images, labels) in enumerate(data_generator):\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "        for image in images:\n",
    "            # Crop the image into four quadrants\n",
    "            height, width = image.shape[0], image.shape[1]\n",
    "            half_height, half_width = height // 2, width // 2\n",
    "            quadrants = [\n",
    "                image[:half_height, :half_width, :],\n",
    "                image[:half_height, half_width:, :],\n",
    "                image[half_height:, :half_width, :],\n",
    "                image[half_height:, half_width:, :],\n",
    "            ]\n",
    "            batch_images.extend(quadrants)\n",
    "            batch_labels.extend([labels[i] for i in range(4)])\n",
    "\n",
    "        batch_images = tf.stack(batch_images)\n",
    "        batch_labels = tf.stack(batch_labels)\n",
    "        yield batch_images, batch_labels\n",
    "\n",
    "        # Check if we have generated enough batches (steps_per_epoch)\n",
    "        if i >= steps_per_epoch:\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 423 images belonging to 7 classes.\n",
      "Found 102 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the training dataset using image_dataset_from_directory\n",
    "train_ds = train_datagen.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse',  # For integer labels\n",
    "    shuffle=True,\n",
    "    subset='training',    # Selecting the training set\n",
    "    seed=1337\n",
    ")\n",
    "\n",
    "# Load the validation dataset using image_dataset_from_directory\n",
    "val_ds = train_datagen.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse',  # For integer labels\n",
    "    shuffle=False,        # No need to shuffle the validation set\n",
    "    subset='validation',  # Selecting the validation set\n",
    "    seed=1337\n",
    ")\n",
    "\n",
    "# Check the number of classes in your dataset\n",
    "num_classes = len(train_ds.class_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the EfficientNetB3 model\n",
    "base_model = EfficientNetB3(weights='imagenet', include_top=False, input_shape=image_size + (3,))\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "105/106 [============================>.] - ETA: 0s - loss: 1.9403 - accuracy: 0.2256"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "2 root error(s) found.\n  (0) Unknown:  IndexError: index 3 is out of bounds for axis 0 with size 3\nTraceback (most recent call last):\n\n  File \"c:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 249, in __call__\n    ret = func(*args)\n\n  File \"c:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 645, in wrapper\n    return func(*args, **kwargs)\n\n  File \"c:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 892, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"c:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 822, in wrapped_generator\n    for data in generator_fn():\n\n  File \"C:\\Users\\diosdadj\\AppData\\Local\\Temp\\ipykernel_24952\\3572417806.py\", line 18, in custom_data_generator\n    batch_labels.extend([labels[i] for i in range(4)])\n\n  File \"C:\\Users\\diosdadj\\AppData\\Local\\Temp\\ipykernel_24952\\3572417806.py\", line 18, in <listcomp>\n    batch_labels.extend([labels[i] for i in range(4)])\n\nIndexError: index 3 is out of bounds for axis 0 with size 3\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n  (1) Unknown:  IndexError: index 3 is out of bounds for axis 0 with size 3\nTraceback (most recent call last):\n\n  File \"c:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 249, in __call__\n    ret = func(*args)\n\n  File \"c:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 645, in wrapper\n    return func(*args, **kwargs)\n\n  File \"c:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 892, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"c:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 822, in wrapped_generator\n    for data in generator_fn():\n\n  File \"C:\\Users\\diosdadj\\AppData\\Local\\Temp\\ipykernel_24952\\3572417806.py\", line 18, in custom_data_generator\n    batch_labels.extend([labels[i] for i in range(4)])\n\n  File \"C:\\Users\\diosdadj\\AppData\\Local\\Temp\\ipykernel_24952\\3572417806.py\", line 18, in <listcomp>\n    batch_labels.extend([labels[i] for i in range(4)])\n\nIndexError: index 3 is out of bounds for axis 0 with size 3\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_2]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_145903]\n\nFunction call stack:\ntrain_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\diosdadj\\OneDrive - HP Inc\\Master\\Lung Cancer Project\\lung_tissues_classification\\generator.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diosdadj/OneDrive%20-%20HP%20Inc/Master/Lung%20Cancer%20Project/lung_tissues_classification/generator.ipynb#X61sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m train_generator \u001b[39m=\u001b[39m custom_data_generator(train_ds, steps_per_epoch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diosdadj/OneDrive%20-%20HP%20Inc/Master/Lung%20Cancer%20Project/lung_tissues_classification/generator.ipynb#X61sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Train the model using the custom data generator\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/diosdadj/OneDrive%20-%20HP%20Inc/Master/Lung%20Cancer%20Project/lung_tissues_classification/generator.ipynb#X61sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diosdadj/OneDrive%20-%20HP%20Inc/Master/Lung%20Cancer%20Project/lung_tissues_classification/generator.ipynb#X61sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     train_generator,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diosdadj/OneDrive%20-%20HP%20Inc/Master/Lung%20Cancer%20Project/lung_tissues_classification/generator.ipynb#X61sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mval_ds,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diosdadj/OneDrive%20-%20HP%20Inc/Master/Lung%20Cancer%20Project/lung_tissues_classification/generator.ipynb#X61sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diosdadj/OneDrive%20-%20HP%20Inc/Master/Lung%20Cancer%20Project/lung_tissues_classification/generator.ipynb#X61sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diosdadj/OneDrive%20-%20HP%20Inc/Master/Lung%20Cancer%20Project/lung_tissues_classification/generator.ipynb#X61sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diosdadj/OneDrive%20-%20HP%20Inc/Master/Lung%20Cancer%20Project/lung_tissues_classification/generator.ipynb#X61sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py:1184\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1178\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1179\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1180\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1181\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1182\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1183\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1184\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1185\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1186\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    882\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 885\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    887\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    888\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:917\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    914\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    915\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 917\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    920\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    921\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3036\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   3037\u001b[0m   (graph_function,\n\u001b[0;32m   3038\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   3040\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1960\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1961\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1962\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1963\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1964\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1965\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m     args,\n\u001b[0;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1968\u001b[0m     executing_eagerly)\n\u001b[0;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    592\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    593\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    594\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    595\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    596\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    597\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    598\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    599\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    600\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    604\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     60\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mUnknownError\u001b[0m: 2 root error(s) found.\n  (0) Unknown:  IndexError: index 3 is out of bounds for axis 0 with size 3\nTraceback (most recent call last):\n\n  File \"c:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 249, in __call__\n    ret = func(*args)\n\n  File \"c:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 645, in wrapper\n    return func(*args, **kwargs)\n\n  File \"c:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 892, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"c:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 822, in wrapped_generator\n    for data in generator_fn():\n\n  File \"C:\\Users\\diosdadj\\AppData\\Local\\Temp\\ipykernel_24952\\3572417806.py\", line 18, in custom_data_generator\n    batch_labels.extend([labels[i] for i in range(4)])\n\n  File \"C:\\Users\\diosdadj\\AppData\\Local\\Temp\\ipykernel_24952\\3572417806.py\", line 18, in <listcomp>\n    batch_labels.extend([labels[i] for i in range(4)])\n\nIndexError: index 3 is out of bounds for axis 0 with size 3\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n  (1) Unknown:  IndexError: index 3 is out of bounds for axis 0 with size 3\nTraceback (most recent call last):\n\n  File \"c:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 249, in __call__\n    ret = func(*args)\n\n  File \"c:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 645, in wrapper\n    return func(*args, **kwargs)\n\n  File \"c:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 892, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"c:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 822, in wrapped_generator\n    for data in generator_fn():\n\n  File \"C:\\Users\\diosdadj\\AppData\\Local\\Temp\\ipykernel_24952\\3572417806.py\", line 18, in custom_data_generator\n    batch_labels.extend([labels[i] for i in range(4)])\n\n  File \"C:\\Users\\diosdadj\\AppData\\Local\\Temp\\ipykernel_24952\\3572417806.py\", line 18, in <listcomp>\n    batch_labels.extend([labels[i] for i in range(4)])\n\nIndexError: index 3 is out of bounds for axis 0 with size 3\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_2]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_145903]\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "# Train the model using the custom data generator\n",
    "epochs = 20\n",
    "# Number of steps per epoch for training\n",
    "steps_per_epoch = len(train_ds)\n",
    "\n",
    "# Number of steps for validation\n",
    "validation_steps = len(val_ds)\n",
    "\n",
    "# Create the custom data generator with cropping\n",
    "train_generator = custom_data_generator(train_ds, steps_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model using the custom data generator\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
