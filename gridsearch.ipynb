{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB4\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, accuracy_score\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 494\n",
      "Total labels: 494\n"
     ]
    }
   ],
   "source": [
    "# Define the root directory where your data is stored\n",
    "root_directory = \"dataset_2_final\"\n",
    "\n",
    "# Get the list of subdirectories (categories)\n",
    "subdirectories = [subdir for subdir in os.listdir(root_directory) if os.path.isdir(os.path.join(root_directory, subdir))]\n",
    "\n",
    "# Create lists to store image paths and corresponding labels\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "# Iterate through each subdirectory (category)\n",
    "for label, subdirectory in enumerate(subdirectories):\n",
    "    # Construct the full path to the subdirectory\n",
    "    subdirectory_path = os.path.join(root_directory, subdirectory)\n",
    "\n",
    "    # Get a list of image files in the subdirectory\n",
    "    image_files = [os.path.join(subdirectory_path, filename) for filename in os.listdir(subdirectory_path) if filename.endswith('.jpg')]  # Adjust the file extension as needed\n",
    "    \n",
    "    # Append image paths and labels\n",
    "    image_paths.extend(image_files)\n",
    "    labels.extend([label] * len(image_files))\n",
    "\n",
    "\n",
    "# Now you have the image_paths and labels\n",
    "print(\"Total images:\", len(image_paths))\n",
    "print(\"Total labels:\", len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom data generator that inherits from the Sequence class\n",
    "class CustomDataGenerator(Sequence):\n",
    "    def __init__(self, image_paths, labels, batch_size, image_size):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.num_samples = 4 * len(image_paths)\n",
    "        self.indices = np.arange(self.num_samples)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return int(np.ceil(self.num_samples / self.batch_size))\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Determine the range of indices for the current batch\n",
    "        batch_indices = self.indices[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "\n",
    "\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "\n",
    "        # Iterate over indices in the current batch\n",
    "        for batch_index in batch_indices:\n",
    "\n",
    "            # Calculate the index of the original image and the piece index within it\n",
    "            image_index = batch_index // 4   # Divide by 4 to get original image index\n",
    "            piece_index = batch_index % 4    # Modulus 4 to get piece index\n",
    "\n",
    "            # Get the image path and original label for the current image\n",
    "            image_path = self.image_paths[image_index]\n",
    "            original_label = self.labels[image_index]\n",
    "            image_pieces = self.load_image(image_path)\n",
    "\n",
    "            # Load and split the image into pieces\n",
    "            piece = image_pieces[piece_index]\n",
    "            batch_images.append(piece)\n",
    "            batch_labels.append(original_label)\n",
    "\n",
    "\n",
    "        batch_images = np.array(batch_images)\n",
    "        batch_labels = np.array(batch_labels)\n",
    "\n",
    "        return batch_images, batch_labels\n",
    "\n",
    "    def load_image(self, image_path):    \n",
    "\n",
    "        # Load the original image using Pillow\n",
    "        original_image = Image.open(image_path)\n",
    "        original_image_array = np.array(original_image)\n",
    "\n",
    "        # Split the original image into 4 pieces\n",
    "        h, w, c = original_image_array.shape\n",
    "        h_half, w_half = h // 2, w // 2\n",
    "       \n",
    "        image_pieces = [\n",
    "            original_image_array[:h_half, :w_half],\n",
    "            original_image_array[:h_half, w_half:],\n",
    "            original_image_array[h_half:, :w_half],\n",
    "            original_image_array[h_half:, w_half:]\n",
    "        ]\n",
    "        \n",
    "        # Resize the image pieces and provide the shape of the output\n",
    "        resized_images = [tf.image.resize(piece, self.image_size) for piece in image_pieces]\n",
    "\n",
    "        #Shape of resized image list\n",
    "        #print(np.array(resized_images).shape)    \n",
    "        \n",
    "        return resized_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 16\n",
    "image_size = (380, 380)  # Adjust the image size based on your model's input requirements\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(image_paths, labels, test_size=0.2, random_state=42)\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(train_paths, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data generators for training, validation, and test sets\n",
    "train_generator = CustomDataGenerator(train_paths, train_labels, batch_size, image_size)\n",
    "val_generator = CustomDataGenerator(val_paths, val_labels, batch_size, image_size)\n",
    "test_generator = CustomDataGenerator(test_paths, test_labels, batch_size, image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model(hp):\n",
    "\n",
    "    base_model = EfficientNetB4(include_top=False, weights='imagenet', pooling='avg')\n",
    "\n",
    "    # Introduce a layer of data augmentation\n",
    "    data_augmentation = Sequential([\n",
    "        preprocessing.RandomRotation(0.2),\n",
    "        preprocessing.RandomFlip(\"horizontal\"),\n",
    "        preprocessing.RandomZoom(0.2),\n",
    "        preprocessing.RandomContrast(0.2),\n",
    "        preprocessing.RandomTranslation(0.2, 0.2),\n",
    "        preprocessing.RandomHeight(0.2),\n",
    "        preprocessing.RandomWidth(0.2),\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Freeze all layers in the base model\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    # Unfreeze the last 10 layers in the base model for fine-tuning\n",
    "    for layer in base_model.layers[-10:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # Capa de entradas\n",
    "    entradas = layers.Input((380, 380, 3))\n",
    "    # Capa de augmentation\n",
    "    x = data_augmentation(entradas)\n",
    "    # Pass the augmented images through the base model\n",
    "    x = base_model(x)\n",
    "\n",
    "    # TUNE the number of units in the dense layer\n",
    "    hp_units = hp.Int('units', min_value=128, max_value=512, step=128)\n",
    "    x = layers.Dense(units=hp_units, activation='relu')(x)\n",
    "    # Add another dense layer\n",
    "    salidas = layers.Dense(7, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=entradas, outputs=salidas)\n",
    "    \n",
    "    #TUNE learning rate\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log'))\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner.tuners import RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 380, 380, 3)]     0         \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "efficientnetb4 (Functional)  (None, 1792)              17673823  \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               229504    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 17,904,230\n",
      "Trainable params: 2,545,671\n",
      "Non-trainable params: 15,358,559\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the RandomSearch tuner\n",
    "tuner = RandomSearch(\n",
    "    create_keras_model,  # Function to build the model\n",
    "    objective='val_accuracy',  # Metric to optimize\n",
    "    max_trials=15,  # Number of hyperparameter combinations to try\n",
    "    directory='random_search',  # Directory to store results\n",
    "    project_name='efficientenet_tuning'  # Name of the tuning project\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [01h 23m 35s]\n",
      "val_accuracy: 0.6645569801330566\n",
      "\n",
      "Best val_accuracy So Far: 0.6645569801330566\n",
      "Total elapsed time: 06h 56m 12s\n",
      "\n",
      "Search: Running Trial #6\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "384               |256               |units\n",
      "0.0064236         |0.00063324        |learning_rate\n",
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 380, 380, 3)]     0         \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "efficientnetb4 (Functional)  (None, 1792)              17673823  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 384)               688512    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 2695      \n",
      "=================================================================\n",
      "Total params: 18,365,030\n",
      "Trainable params: 3,006,471\n",
      "Non-trainable params: 15,358,559\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "79/79 [==============================] - 375s 5s/step - loss: 2.1987 - accuracy: 0.3070 - val_loss: 10.7614 - val_accuracy: 0.3829\n",
      "Epoch 2/15\n",
      "61/79 [======================>.......] - ETA: 1:14 - loss: 1.6751 - accuracy: 0.3576"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\diosdadj\\OneDrive - HP Inc\\Master\\Lung Cancer Project\\lung_tissues_classification\\gridsearch.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diosdadj/OneDrive%20-%20HP%20Inc/Master/Lung%20Cancer%20Project/lung_tissues_classification/gridsearch.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Search for the best hyperparameters\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/diosdadj/OneDrive%20-%20HP%20Inc/Master/Lung%20Cancer%20Project/lung_tissues_classification/gridsearch.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tuner\u001b[39m.\u001b[39;49msearch(train_generator, epochs\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49mval_generator)\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:230\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m--> 230\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_run_and_update_trial(trial, \u001b[39m*\u001b[39mfit_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs)\n\u001b[0;32m    231\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_trial_end(trial)\n\u001b[0;32m    232\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_search_end()\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:270\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_run_and_update_trial\u001b[39m(\u001b[39mself\u001b[39m, trial, \u001b[39m*\u001b[39mfit_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs):\n\u001b[0;32m    269\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 270\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_and_update_trial(trial, \u001b[39m*\u001b[39mfit_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs)\n\u001b[0;32m    271\u001b[0m         trial\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m trial_module\u001b[39m.\u001b[39mTrialStatus\u001b[39m.\u001b[39mCOMPLETED\n\u001b[0;32m    272\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:235\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_and_update_trial\u001b[39m(\u001b[39mself\u001b[39m, trial, \u001b[39m*\u001b[39mfit_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs):\n\u001b[1;32m--> 235\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_trial(trial, \u001b[39m*\u001b[39mfit_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs)\n\u001b[0;32m    236\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mget_trial(trial\u001b[39m.\u001b[39mtrial_id)\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mexists(\n\u001b[0;32m    237\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mobjective\u001b[39m.\u001b[39mname\n\u001b[0;32m    238\u001b[0m     ):\n\u001b[0;32m    239\u001b[0m         \u001b[39m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[0;32m    240\u001b[0m         \u001b[39m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[0;32m    241\u001b[0m         \u001b[39m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    243\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe use case of calling \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    244\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    250\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m    251\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras_tuner\\engine\\tuner.py:287\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(model_checkpoint)\n\u001b[0;32m    286\u001b[0m     copied_kwargs[\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m callbacks\n\u001b[1;32m--> 287\u001b[0m     obj_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_and_fit_model(trial, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    289\u001b[0m     histories\u001b[39m.\u001b[39mappend(obj_value)\n\u001b[0;32m    290\u001b[0m \u001b[39mreturn\u001b[39;00m histories\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras_tuner\\engine\\tuner.py:214\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m hp \u001b[39m=\u001b[39m trial\u001b[39m.\u001b[39mhyperparameters\n\u001b[0;32m    213\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_build(hp)\n\u001b[1;32m--> 214\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhypermodel\u001b[39m.\u001b[39mfit(hp, model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    215\u001b[0m tuner_utils\u001b[39m.\u001b[39mvalidate_trial_results(\n\u001b[0;32m    216\u001b[0m     results, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mobjective, \u001b[39m\"\u001b[39m\u001b[39mHyperModel.fit()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m )\n\u001b[0;32m    218\u001b[0m \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py:144\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[1;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, hp, model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    121\u001b[0m     \u001b[39m\"\"\"Train the model.\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \n\u001b[0;32m    123\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[39m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m     \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39mfit(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py:1184\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1178\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1179\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1180\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1181\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1182\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1183\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1184\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1185\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1186\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    882\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 885\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    887\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    888\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:917\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    914\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    915\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 917\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    920\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    921\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3036\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   3037\u001b[0m   (graph_function,\n\u001b[0;32m   3038\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   3040\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1960\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1961\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1962\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1963\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1964\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1965\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m     args,\n\u001b[0;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1968\u001b[0m     executing_eagerly)\n\u001b[0;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    592\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    593\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    594\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    595\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    596\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    597\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    598\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    599\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    600\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    604\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     60\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Search for the best hyperparameters\n",
    "tuner.search(train_generator, epochs=15, validation_data=val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in random_search\\efficientenet_tuning\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 04 summary\n",
      "Hyperparameters:\n",
      "units: 256\n",
      "learning_rate: 0.0006332383741037616\n",
      "Score: 0.6645569801330566\n",
      "\n",
      "Trial 00 summary\n",
      "Hyperparameters:\n",
      "units: 512\n",
      "learning_rate: 0.00040995319944661575\n",
      "Score: 0.655063271522522\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "units: 384\n",
      "learning_rate: 0.003956370567137552\n",
      "Score: 0.6518987417221069\n",
      "\n",
      "Trial 01 summary\n",
      "Hyperparameters:\n",
      "units: 384\n",
      "learning_rate: 0.0009064135578205235\n",
      "Score: 0.6329113841056824\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "units: 384\n",
      "learning_rate: 0.002242865877928494\n",
      "Score: 0.5981012582778931\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "units: 384\n",
      "learning_rate: 0.006423597973604284\n",
      "Results summary\n",
      "Results in random_search\\efficientenet_tuning\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 04 summary\n",
      "Hyperparameters:\n",
      "units: 256\n",
      "learning_rate: 0.0006332383741037616\n",
      "Score: 0.6645569801330566\n",
      "\n",
      "Trial 00 summary\n",
      "Hyperparameters:\n",
      "units: 512\n",
      "learning_rate: 0.00040995319944661575\n",
      "Score: 0.655063271522522\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "units: 384\n",
      "learning_rate: 0.003956370567137552\n",
      "Score: 0.6518987417221069\n",
      "\n",
      "Trial 01 summary\n",
      "Hyperparameters:\n",
      "units: 384\n",
      "learning_rate: 0.0009064135578205235\n",
      "Score: 0.6329113841056824\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "units: 384\n",
      "learning_rate: 0.002242865877928494\n",
      "Score: 0.5981012582778931\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "units: 384\n",
      "learning_rate: 0.006423597973604284\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 380, 380, 3)]     0         \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "efficientnetb4 (Functional)  (None, 1792)              17673823  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               459008    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 1799      \n",
      "=================================================================\n",
      "Total params: 18,134,630\n",
      "Trainable params: 2,776,071\n",
      "Non-trainable params: 15,358,559\n",
      "_________________________________________________________________\n",
      "Best model summary: \n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 380, 380, 3)]     0         \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "efficientnetb4 (Functional)  (None, 1792)              17673823  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               459008    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 1799      \n",
      "=================================================================\n",
      "Total params: 18,134,630\n",
      "Trainable params: 2,776,071\n",
      "Non-trainable params: 15,358,559\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get the best hyperparameters\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "#Print a graph of the search\n",
    "tuner.results_summary()\n",
    "\n",
    "\n",
    "#Print all available information about the grid search\n",
    "tuner.results_summary()\n",
    "\n",
    "\n",
    "# Build and compile the best model\n",
    "best_model = tuner.hypermodel.build(best_hyperparameters)\n",
    "\n",
    "print(\"Best model summary: \")\n",
    "best_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
