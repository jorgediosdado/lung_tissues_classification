{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\diosdadj\\OneDrive - HP Inc\\Master\\Lung Cancer Project\\lung_tissues_classification\\efficientnet_cropped_images.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diosdadj/OneDrive%20-%20HP%20Inc/Master/Lung%20Cancer%20Project/lung_tissues_classification/efficientnet_cropped_images.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/diosdadj/OneDrive%20-%20HP%20Inc/Master/Lung%20Cancer%20Project/lung_tissues_classification/efficientnet_cropped_images.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diosdadj/OneDrive%20-%20HP%20Inc/Master/Lung%20Cancer%20Project/lung_tissues_classification/efficientnet_cropped_images.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diosdadj/OneDrive%20-%20HP%20Inc/Master/Lung%20Cancer%20Project/lung_tissues_classification/efficientnet_cropped_images.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m layers\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\__init__.py:41\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msix\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_six\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     44\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\__init__.py:40\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtraceback\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[0;32m     37\u001b[0m \u001b[39m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[0;32m     41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tensorflow \u001b[39mas\u001b[39;00m _pywrap_tensorflow\n\u001b[0;32m     43\u001b[0m \u001b[39m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \n\u001b[0;32m     45\u001b[0m \u001b[39m# Bring in subpackages.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:35\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m config_pb2\n\u001b[0;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m rewriter_config_pb2\n\u001b[1;32m---> 35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tfe\n\u001b[0;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m tf2\n\u001b[0;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclient\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tf_session\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py:28\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m print_function\n\u001b[0;32m     27\u001b[0m \u001b[39m# pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tensorflow\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_pywrap_tfe\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:64\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39m# pylint: disable=wildcard-import,g-import-not-at-top,line-too-long,undefined-variable\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_pywrap_tensorflow_internal\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     65\u001b[0m \u001b[39m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[39m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \n\u001b[0;32m     70\u001b[0m \u001b[39m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.applications import EfficientNetB4\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"crop_dataset_2\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4001 files belonging to 7 classes.\n",
      "Using 3201 files for training.\n",
      "Found 4001 files belonging to 7 classes.\n",
      "Using 800 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "\n",
    "image_size = (380, 380)\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    dataset_name,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    label_mode = 'int',\n",
    "    labels = 'inferred',\n",
    "    seed=1336,\n",
    "    image_size=image_size,\n",
    "    interpolation = 'bilinear',\n",
    "    batch_size=batch_size,\n",
    "    shuffle = True,\n",
    ")\n",
    "\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    dataset_name,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    label_mode = 'int',\n",
    "    labels = 'inferred',\n",
    "    seed=1336,\n",
    "    image_size=image_size,\n",
    "    interpolation = 'bilinear',\n",
    "    batch_size=batch_size,\n",
    "    shuffle = True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aca_bd', 'aca_md', 'aca_pd', 'nor', 'scc_bd', 'scc_md', 'scc_pd']\n"
     ]
    }
   ],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the number of samples\n",
    "num_train_samples = 0\n",
    "for images, labels in train_ds:\n",
    "    num_train_samples += images.shape[0]\n",
    "\n",
    "num_val_samples = 0\n",
    "for images, labels in val_ds:\n",
    "    num_val_samples += images.shape[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 380, 380, 3)\n",
      "(16,)\n"
     ]
    }
   ],
   "source": [
    "# Print the dimensions of the training dataset\n",
    "for images, labels in train_ds:\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[185.87535    86.68906   134.0838   ]\n",
      "  [253.82964   186.9737    212.45361  ]\n",
      "  [205.88089   168.0547    181.34418  ]\n",
      "  ...\n",
      "  [201.7577     83.747185  180.0312   ]\n",
      "  [206.39468    88.040146  183.5851   ]\n",
      "  [202.28105    81.145325  176.9327   ]]\n",
      "\n",
      " [[189.37189    92.87812   146.49724  ]\n",
      "  [237.24446   172.47021   203.45152  ]\n",
      "  [212.58519   175.59003   194.89404  ]\n",
      "  ...\n",
      "  [194.268      84.38503   175.5311   ]\n",
      "  [194.75212    88.82352   174.74109  ]\n",
      "  [200.53392    96.75838   183.14066  ]]\n",
      "\n",
      " [[193.7867     99.40789   164.42383  ]\n",
      "  [224.69183   158.77078   201.77078  ]\n",
      "  [211.25693   168.04709   199.17937  ]\n",
      "  ...\n",
      "  [204.31642   104.1059    190.07951  ]\n",
      "  [205.4523    107.10062   193.07913  ]\n",
      "  [205.9307    113.32067   197.85182  ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[201.48425   164.11021   204.0841   ]\n",
      "  [199.11005   143.97842   193.93564  ]\n",
      "  [202.6842    122.289474  190.63158  ]\n",
      "  ...\n",
      "  [119.472275    7.1852465 167.5244   ]\n",
      "  [146.89171    16.028015  202.39122  ]\n",
      "  [162.99103    27.521528  199.43765  ]]\n",
      "\n",
      " [[206.77567   166.42728   211.3345   ]\n",
      "  [197.75208   139.56784   193.96605  ]\n",
      "  [202.01665   121.62192   189.99518  ]\n",
      "  ...\n",
      "  [106.30638     3.8085356 149.21805  ]\n",
      "  [156.17784    29.779215  204.4934   ]\n",
      "  [163.24005    26.195032  192.68732  ]]\n",
      "\n",
      " [[211.18636   165.55469   214.23903  ]\n",
      "  [198.97998   137.86217   195.28336  ]\n",
      "  [207.2211    126.489784  194.90044  ]\n",
      "  ...\n",
      "  [128.95781    22.809904  166.72112  ]\n",
      "  [142.47983    15.123315  183.12479  ]\n",
      "  [151.81976    14.477598  175.50385  ]]], shape=(380, 380, 3), dtype=float32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_ds:\n",
    "    print(images[0])\n",
    "    print(labels[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 3201\n",
      "Number of validation samples: 800\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training samples:\", num_train_samples)\n",
    "print(\"Number of validation samples:\", num_val_samples)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each EfficientNet variant has a specific input size requirement. Here are the input sizes for some popular EfficientNet models:\n",
    "\n",
    "EfficientNetB0: (224, 224, 3)\n",
    "EfficientNetB1: (240, 240, 3)\n",
    "EfficientNetB2: (260, 260, 3)\n",
    "EfficientNetB3: (300, 300, 3)\n",
    "EfficientNetB4: (380, 380, 3)\n",
    "EfficientNetB5: (456, 456, 3)\n",
    "EfficientNetB6: (528, 528, 3)\n",
    "EfficientNetB7: (600, 600, 3)\n",
    "These sizes represent the input dimensions (height, width, number of channels) that the models expect. Therefore, when using any EfficientNet variant, you need to resize your images to match the corresponding input size before feeding them into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 380, 380, 3)]     0         \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 380, 380, 3)       0         \n",
      "_________________________________________________________________\n",
      "efficientnetb4 (Functional)  (None, 1792)              17673823  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               918016    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 3591      \n",
      "=================================================================\n",
      "Total params: 18,595,430\n",
      "Trainable params: 3,236,871\n",
      "Non-trainable params: 15,358,559\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = EfficientNetB4(include_top = False ,weights='imagenet', pooling='avg')\n",
    "\n",
    "\n",
    "# Introduce a layer of data augmentation\n",
    "data_augmentation = Sequential([\n",
    "    preprocessing.RandomRotation(0.2),\n",
    "    preprocessing.RandomFlip(\"horizontal\"),\n",
    "    preprocessing.RandomZoom(0.2),\n",
    "    preprocessing.RandomContrast(0.2),\n",
    "    preprocessing.RandomTranslation(0.2, 0.2),    \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Freeze all layers in the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Unfreeze the last 10 layers in the base model for fine-tuning\n",
    "\n",
    "for layer in base_model.layers[-10:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "#Capa de entradas. No se si eficientnet ya espera 225x225 o es 224 x 224. En realidad la efficient net ya tiene un parametro de input shape...\n",
    "# Capa de entradas\n",
    "\n",
    "entradas = layers.Input((380, 380, 3))\n",
    "\n",
    "# Apply data augmentation to the input images\n",
    "x = data_augmentation(entradas)\n",
    "\n",
    "\n",
    "# Pass the augmented images through the base model\n",
    "x = base_model(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Add a dense layer\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "# Add another dense layer\n",
    "salidas = layers.Dense(7, activation='softmax')(x)\n",
    "\n",
    "modelo = Model(inputs = entradas, outputs = salidas)\n",
    "\n",
    "modelo.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "modelo.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 18595430\n"
     ]
    }
   ],
   "source": [
    "num_params = modelo.count_params()\n",
    "print(\"Number of parameters:\", num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_date = 'old_models/' +  datetime.now().strftime(\"%Y_%m_%d-%I_%M_%S_%p\")\n",
    "os.mkdir(folder_date)\n",
    "filepath_model = folder_date + '/my_best_model.epoch{epoch:02d}-loss{val_loss:.2f}.hdf5'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "filepath=filepath_model,\n",
    "save_weights_only=False,\n",
    "monitor='val_accuracy',\n",
    "mode='max',\n",
    "save_freq=\"epoch\",\n",
    "save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, mode='auto', verbose = 1, )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para hacer un training con weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Calculate class weights\n",
    "# # class_weights = tf.class_weight.compute_class_weight('balanced', tf.unique(train_ds.labels)[0], train_ds.labels)\n",
    "\n",
    "# # Convert class weights to dictionary\n",
    "# # class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# # history = modelo.fit(\n",
    "# #     train_ds, epochs=epochs, validation_data=val_ds, callbacks=[model_checkpoint_callback], class_weight=class_weights_dict\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "201/201 [==============================] - 183s 724ms/step - loss: 1.2102 - accuracy: 0.5608 - val_loss: 1.1781 - val_accuracy: 0.6137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diosdadj\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n",
      "201/201 [==============================] - 138s 683ms/step - loss: 0.9340 - accuracy: 0.6614 - val_loss: 1.1379 - val_accuracy: 0.6525\n",
      "Epoch 3/20\n",
      "201/201 [==============================] - 146s 723ms/step - loss: 0.8512 - accuracy: 0.6973 - val_loss: 0.6286 - val_accuracy: 0.7625\n",
      "Epoch 4/20\n",
      "201/201 [==============================] - 126s 624ms/step - loss: 0.7691 - accuracy: 0.7266 - val_loss: 0.7270 - val_accuracy: 0.7500\n",
      "Epoch 5/20\n",
      "201/201 [==============================] - 129s 640ms/step - loss: 0.7117 - accuracy: 0.7420 - val_loss: 0.5954 - val_accuracy: 0.7713\n",
      "Epoch 6/20\n",
      "201/201 [==============================] - 151s 750ms/step - loss: 0.6778 - accuracy: 0.7538 - val_loss: 0.7137 - val_accuracy: 0.7650\n",
      "Epoch 7/20\n",
      "201/201 [==============================] - 138s 684ms/step - loss: 0.6257 - accuracy: 0.7716 - val_loss: 0.5871 - val_accuracy: 0.7800\n",
      "Epoch 8/20\n",
      "201/201 [==============================] - 140s 693ms/step - loss: 0.5771 - accuracy: 0.7898 - val_loss: 0.5382 - val_accuracy: 0.8263\n",
      "Epoch 9/20\n",
      "201/201 [==============================] - 130s 648ms/step - loss: 0.5672 - accuracy: 0.7929 - val_loss: 0.6454 - val_accuracy: 0.7663\n",
      "Epoch 10/20\n",
      "201/201 [==============================] - 122s 607ms/step - loss: 0.5411 - accuracy: 0.8069 - val_loss: 0.5896 - val_accuracy: 0.8138\n",
      "Epoch 11/20\n",
      "201/201 [==============================] - 121s 603ms/step - loss: 0.5089 - accuracy: 0.8229 - val_loss: 0.5572 - val_accuracy: 0.8062\n",
      "Epoch 12/20\n",
      "201/201 [==============================] - 122s 605ms/step - loss: 0.4812 - accuracy: 0.8291 - val_loss: 0.4846 - val_accuracy: 0.8288\n",
      "Epoch 13/20\n",
      "201/201 [==============================] - 124s 614ms/step - loss: 0.4666 - accuracy: 0.8338 - val_loss: 0.5734 - val_accuracy: 0.8125\n",
      "Epoch 14/20\n",
      "201/201 [==============================] - 140s 692ms/step - loss: 0.4767 - accuracy: 0.8285 - val_loss: 0.5302 - val_accuracy: 0.8125\n",
      "Epoch 15/20\n",
      "201/201 [==============================] - 121s 602ms/step - loss: 0.4168 - accuracy: 0.8529 - val_loss: 0.4229 - val_accuracy: 0.8500\n",
      "Epoch 16/20\n",
      "201/201 [==============================] - 122s 603ms/step - loss: 0.4069 - accuracy: 0.8541 - val_loss: 0.7012 - val_accuracy: 0.7812\n",
      "Epoch 17/20\n",
      "201/201 [==============================] - 121s 600ms/step - loss: 0.3686 - accuracy: 0.8638 - val_loss: 0.5804 - val_accuracy: 0.8238\n",
      "Epoch 18/20\n",
      "201/201 [==============================] - 122s 608ms/step - loss: 0.3947 - accuracy: 0.8579 - val_loss: 0.3687 - val_accuracy: 0.8687\n",
      "Epoch 19/20\n",
      "201/201 [==============================] - 131s 650ms/step - loss: 0.3993 - accuracy: 0.8604 - val_loss: 0.4430 - val_accuracy: 0.8537\n",
      "Epoch 20/20\n",
      "201/201 [==============================] - 135s 669ms/step - loss: 0.3604 - accuracy: 0.8704 - val_loss: 0.4724 - val_accuracy: 0.8512\n",
      "50/50 [==============================] - 28s 560ms/step - loss: 0.4724 - accuracy: 0.8512\n"
     ]
    }
   ],
   "source": [
    "# Start an MLflow run\n",
    "mlflow.start_run()\n",
    "mlflow.set_experiment('scientific_data')\n",
    "# Log the name of the dataset\n",
    "mlflow.log_param(\"dataset_name\", dataset_name)\n",
    "# Log the number of samples in the dataset\n",
    "#   mlflow.log_param(\"num_samples_train\", num_train_samples)\n",
    "#   mlflow.log_param(\"num_samples_train\", num_val_samples)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "history = modelo.fit(\n",
    "    train_ds, epochs=epochs, validation_data=val_ds, callbacks=[model_checkpoint_callback]\n",
    ")\n",
    " \n",
    "# Evaluate the model\n",
    "loss, accuracy = modelo.evaluate(val_ds)\n",
    "\n",
    "#Logging\n",
    "\n",
    "\n",
    "mlflow.log_param(\"epochs\", epochs)\n",
    "mlflow.log_param(\"batch_size\", batch_size)\n",
    "\n",
    "mlflow.log_param(\"parameters\", num_params)\n",
    "\n",
    "mlflow.log_metric(\"val_loss\", loss)\n",
    "mlflow.log_metric(\"val_accuracy\", accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardo el modelo en la última epoch. Esto \n",
    "modelo.save(folder_date + '/modelo_entrenado.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'old_models/2023_08_18-09_34_22_AM/accuracy.png'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_date + \"/accuracy.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "\n",
    "#  \"Accuracy\"\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "\n",
    "plt.text(len(history.history['accuracy'])-1, history.history['accuracy'][-1], \n",
    "         f\"Acc: {history.history['accuracy'][-1]:.4f}\", ha='center', va='bottom')\n",
    "plt.text(len(history.history['val_accuracy'])-1, history.history['val_accuracy'][-1], \n",
    "         f\"Val Acc: {history.history['val_accuracy'][-1]:.4f}\", ha='center', va='top')\n",
    "\n",
    "\n",
    "plt.savefig(folder_date + \"/accuracy.png\")\n",
    "plt.show()\n",
    "\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "\n",
    "plt.text(len(history.history['loss'])-1, history.history['loss'][-1], \n",
    "         f\"Loss: {history.history['loss'][-1]:.4f}\", ha='center', va='bottom')\n",
    "plt.text(len(history.history['val_loss'])-1, history.history['val_loss'][-1], \n",
    "         f\"Val Loss: {history.history['val_loss'][-1]:.4f}\", ha='center', va='top')\n",
    "\n",
    "plt.savefig(folder_date + \"/loss.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preguntas, cómo puedo gestionar datos que vienen de dos distribuciones distintas?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a section to evaluate the model on te test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 1, 1, 1, 1, 1, 0, 0, 5, 3, 3, 2, 3, 2, 2, 2, 2, 3, 4, 4, 4, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 2, 3, 6, 3, 3, 3, 3, 4, 5, 4, 4, 4, 4, 4, 1, 4, 4, 5, 4, 1, 4, 4, 4, 0, 5, 6, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 5, 6, 6, 5, 6, 2, 6, 2, 6, 6, 6, 6]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVIAAAEZCAYAAADFSq1AAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAAsTAAALEwEAmpwYAAA6BUlEQVR4nO2deZhU1bW33183zQwNTSOzghPGGUEiGhUjiUNi0EzqNRrjTRCvs9eY5DrHL0SjiSZKVPAqxtlcNThEwQlnlEFAUXECQWmUBhqQbuhpfX+c01A01V3VXVWnquj1Ps95us4+56y19j7Vq/a4tswMx3Ecp/UUZNsAx3GcfMcdqeM4Toq4I3Ucx0kRd6SO4zgp4o7UcRwnRdyROo7jpIg7UicSJF0lqcVz7SQNDp/dORN2tdCWqyR9uxXPHS/pokzYFMrPmTJqq7gjdXKdwcCVQC44iSuBFjtS4HggY46U3CqjNok70u0YSR2ybYPjtAXckeYRkvaT9JikVZKqJC2S9Lvw2gxJr0o6TtLbkjYB/xVeGynpOUlfS9og6XlJIxvJniLpc0kHS5olaaOkJZLObYWdwyS9Esr4QtLlgOLcd46kNyStllQhaaak78VcHw28GJ4+K8nCY3R4/SRJL0haGebtbUk/j6PnfEnvh2W2RtJsSSc0uueHof7K0JZ/Stox5npDt8SlMXZclURZTAF+DgyIeW5JzPVSSbeG5bRJ0geSxjWS0VfS3ZKWh/eUSXpS0g6JysiJhnbZNsBJjtDxzQA+Bi4EPgd2A/aNuW134G/ANcCnwGpJ+wIvAe8BpwMG/BZ4SdJBZjY/5vnuwEPAdaGek4C/SVpvZlOStLMUeAFYQeBANgG/BnaMc/tg4A5gCcF38TjgSUnHmtnTwFzgbGAicB4wK3zuvfDvzsD/AdcC9cBhwB2SOpnZbaE9pwB/Bn4PvAJ0CsusJMbm8cCtwF3hfd2Aq8Iy2tfM1gOjgDeAKcDt4aOfJ1Ek1wC9gQOBH4Rpm0K93YHXQpuuAhYDRwG3SupgZjeH998D7ERQjsuAPsCRQOckysiJAjPzIw8O4GWCf6LOTVyfQeBM9m+U/n9ABdAjJq07sBp4NCZtCoGTPanR888CnwFK0s4/ANXAjjFpXYDy4OvW5HMFBM50OjA1Jn10aNeYBHobnp8MzI9JvwWY28xzXYG1wJ2N0geH+bggJs2A/9eKdzcF+DxO+uXARmC3RumTw/JqF55/DZzXjPykysiPzB3etM8DJHUGDgHuM7PKZm5dYmbzGqUdBjxpZhUNCWa2DngcOLzRvXXAI43SHiSoTQ5I0txRwEwzWxqjbwPwROMbJQ0Pm6hfArVADfAdYGgyiiTtJukBSV+Ez9YAv2z0/Cxgf0k3SxoTlmVje7sD90lq13AQ1DY/ICi/THE08CawuJHuaUAvYM+YPPw67KLYR9I23SROdnFHmh/0JHhXiZqSZXHSSppIXxHKjWWNmdU0Svsy/JusI+0X80w8OQBIGgQ8H9p3LnAwQfP3GaBjIiWSuhLUlvcj6Ko4NHz+TiB2kO0fwFnANwkc1GpJj0oaHF7fIfz7HFucccOxD4FDyxQ7EDjqxnr/GV5v0H0iwQ/fJcAC4AtJV0jy/98cwftI84M1BM32RM4s3jzN1UDfOOl9w2ux9JRU1MiZ9gn/fpGMoQROu0+c9MZpRwPFwE/NbPMPRJwaY1OMIug3PNTMXo15fqvvtAVt39uB2yX1BL5L0Gf6EIFzXRXeejqwMI6e9Una0xpWAV8B5zdxfRGAmX1F0A96tqShBH3PVwMrCfp2nSzjv2h5QNicfxX4maROLXz8JeB7kro1JISfjwuvxVII/KhR2knAUpJ3pG8AB4U1zgZ9XUJ9sTQ4zJqY+3Yn6MKIZVP4t3G+4z3fExjblGFmtsbMHgIeBvYOk18ncJa7mtnsOMeiGBHVcexIhk1NPPcMsAewtAnd2zhxM1tkZv9D8OPakIemysiJCK+R5g8XEzi+NyT9maCZvzPB4FJzU5SuAb4PPC/pOoJa628IHNHvG927HvhTOPL+EXAyMAY4PazZJcONBNOupofTgxpG7asa3fccQb/oP8L89COoZS1l6x/4D8P7zpC0OpS3iMABrgMmSrqSYEDrMoJBmuKGhyVNCvP1BkHtb3fgVIJBLcxsnaRfh3J6A08TDD4NIOhDnmFm94fi3iP4UXqGwJEtN7PlSZTJe0CJpLOA2cBGM3snLKsTgVck3RjmqwuBcz3UzMZKKg7L6j6CPtsagh+Lng15aKqM4jliJ0Nke7TLj+QPYBjBoE0FgWP6APhNeG0G8GoTz32T4J/xa2ADQd/kyEb3TCFwzgcTDG5sJBitb3K0uBk7DyCYarSRoCZ7OYGTtEb3/TTMw0aCZvVJoR1LGt13JsF0rlqCH4LRYfq3gbfDsviEYPrPVbF6CJrBMwic6CaCKUY3At0b6TiWYD7mulDexwT9rXvG3HMIMCe014CrkiyPLsADBM7XYvNH4BBvDO2qDu18hXC2AEF/7+1h+Xwd2jcL+I9kysiPaA6FL8Fp44QTx8eY2cBs2+I4+Yb3kTqO46SI95E6SdN4RDwOddaGmjjhfM7C5u4xs9qIzHGyiNdIHQDM7PQkmvWN5zs2PrZZ576dczgJyiRmvqqzHeM1UqclHJjg+uJIrMgd5pC4TJIZ1XfyHB9siqG0pNAGDyrKiu4PFyQ7D337QkXZ/S23Gm95R8lGNlBtm1Ja4nrUEV1s1eq6pO6ds2DTNDM7OhV9yeA10hgGDyrirWmDEt+YAY7qv39W9GabdqXxFkFFR+2KeKtZnUzxpj2fsozy1XW8OS25ySVF/T4pTVlhErgjdRwnzzDqrD7bRmyFO1LHcfIKA+rjhpXIHu5IHcfJO+rxGqnjOE6rMYy6HBskd0fqOE5eYUCN10gdx3FSw/tIHcdxUsDAm/aO4zipklsNe3ekjuPkGYZR5017x3Gc1mMGNbnlRz36U2v484WD+Ok+ezHuiC27/t5zQ1/+44A9OWvMUM4aM5S3nu/WjIT0MWL0Ou545QPueu19fnpOtMsds6n7/Cvf5b7nXmTiw69FqhfabplnU/fWiLokj4SSpDslfSXp3TjXLpZk4dY7zeKOtBV898TV/OG+T7dJP+FXK7n1uUXc+twiRh6Z+e1yCgqMsyd8wWWnDOFXo4dyxNgKdtxtY8b1Zls3wHNP9OeKc4ZHpq+Btlrm2X7fsRhQb8kdSTCFYEfbrQg3b/wOwR5iCclrRyrpdEm3JHHfVZIuTpfefQ7aQLeeyUWfySRDh1WyfEl7ViztQG1NATOm9mDUUWu3e90AC+eWsH5t9JG62mqZZ/t9NyZdNVIze5lttyWHYB+tS4i/xfk25LUjzTWeuKs3448cyp8vHMT6imYDp6eFXn1rWLm8/ebz8rIiSvvVNPPE9qE7m7TVMs+l9220yJGWSpodc4xLJF/SD4AvzGx+sjZF5kgl/UvSHEkLGzIj6WhJcyXNl/R8mDZS0uuS3g7/Dm1eMoMkPSNpUbgtb4O+S8O054AmZUga11DIK1e1vpb5/Z+Xc9cb7/H3ZxdR0qeGSVf3b7WsZFGcH9yoptdlU3c2aatlnmvvu96U1AGUm9mImGNSc3IldQYuBa5oiT1RjtqfYWarJXUCZkmaCkwGDjOzxZJKwvs+CNNqJY0BJgA/akbuSGBvoDKU+xTBj9ZJBNsXtwPmEkQz34awYCcBjNivY6u/Gj17bwkQfMwpq7nitCGtFZU05WVF9O5fvfm8tF8Nq1ZE09zNpu5s0lbLPJfedz2iuvmtslJhF2AIMD/YkouBwFxJI81sRVMPRdm0P0/SfGAmMAgYB7xsZosBzKyhn6IY+Gc4inYjsFcCuc+a2SozqwIeBb4FHAo8ZmaVZrYOeDz92dmaVV9u+U16/eliBg/NfEf8onmdGTCkmj6DNtGuqJ7RYyuYOb0443qzrTubtNUyz7X33YIaaYsws3fMbAczG2xmg4HPgQOac6IQUY1U0mhgDDDKzColzQDmE7/JfQ3wopmdEG4cNiOB+Ma1SAMUJz1t/PGsnVjwRlfWrm7HKcP35NT/XsGCN7ryycJOSNBnYDXn/WlZptRvpr5OTLx0ABPu/5SCQpj+YAmffdgx43qzrRvgkgkL2Gf4arr3qOHup1/ivtt2YfrU5KKmp0JbLfNsv+9YGvpI04GkB4DRBH2pnwNXmtn/tlhOFHs2SRoL/NLMjpO0BzAPOBX4CzFN+7Dp/xhwr5k9Iukq4PTwlyGe3NMJmv57A1XAm8AZBCvIpgDfZEvT/nYzu6E5O0fs19F8q5FoadfXtxppS7xpz7POVqfkBffYt6NNfjy5H83Dhnwyx8xGpKIvGaLqI30GGC9pAbCIoHm/kqB5/6ikAuArgnlbfwLulnQR8EISsl8F7gF2Be43s9kAkh4icNifAa+kNTeO42SNIEJ+bk04isSRmtkm4JgmLj/d6N43gN1jki5vRu4UgppnvGt/AP7QEjsdx8kP0tW0Txe+1t5xnLzCTNRY5udpt4S8cKSSjgKua5S82MxOyIY9juNkj2CwqQ027VPFzKYB07Jth+M4uYCoM3ekjuM4rabNDjY5juOkk7pWTLbPJO5IHcfJKwxRY7nlunLLGsdxnAT4YJPjOE6KGPKmfS6z8Kve7P3X/8qK7g231ia+KUPsftZbWdPdlsnm8th8Xxrrg02O4zgpYIZPf3Icx0kNUe9LRB3HcVqPAdU+au84jtN6jNYFbc4k7kgdx8k7fPqT4zhOCgT72rsjdRzHSYHk9qyPEnekjuPkFV4jdRzHSZFcDOycW27dcRwnCeqsIKkjEZLulPRVuP17Q9r1kj6QtEDSY5J6JJLjjtRxnLwiiEeqpI4kmAIc3SjtWWBvM9sX+BD4XSIh3rRPA93ab+LqMTPYtddqAC5/9gjmr+gbie4ez6+g+LWVAGwa0IkvT9sZK4rm93HE6HWMv2Y5hQXG0w+U8PAt0a0dP//Kdxl56EoqVrfn7J8eEple8HxnI99bk74I+Wb2sqTBjdKmx5zOBH6cSI7XSNPAbw9/ldc+G8QP7jmZH973Uz5d3TMSve0qqun54gqW/nYvPrtiH1QP3WavikR3QYFx9oQvuOyUIfxq9FCOGFvBjrttjEQ3wHNP9OeKc4ZHpq8Bz3d28h1LMNikpA6gVNLsmGNcC9WdQaOdjuPRJhyppMGxfSDppEv7aoYPKOORhd8AoLa+kPXVHTKhKj71oJp6qDNUXUdtcftI1A4dVsnyJe1ZsbQDtTUFzJjag1FHrY1EN8DCuSWsX1sUmb4GPN/ZyXcsQWDnwqQOoNzMRsQck5LVI+lSoBa4L9G93rRPkYHd17GmqhP/7zsvMrR0Fe99Vcq1L32LqtrMf9lre7RnzZi+7HzpPOqLCqj8RjGVexZnXC9Ar741rFy+xWmXlxWxxwGVkejOJp7vgGznO9Nh9CT9HPg+cKSZWaL7M2qNpH9JmiNpYUOVWtLRkuZKmi/p+TBtpKTXJb0d/h3ajMzTQ7lPSFos6RxJF4XPzpRUEt43PNTxBnB2M/LGNVT7ays3tDiP7Qrq+cYOK3lowV785IGfUFVTxH+OeLvFclpDwYZaus5fw+Jr9uPTa/enoLqObm+WR6JbcfrxE3/d8h/P9xayle8gjJ6SOlqDpKOB3wA/MLOkfi0y3bQ/w8yGAyOA8yT1ASYDPzKz/YCfhPd9ABxmZsOAK4AJCeTuDfwHMBL4A1AZPvsGcFp4z13AeWY2qjlBZjapodrfrnOXFmdwxddd+fLrrrzzZdDxPv3jndlzh5UtltMaOn+wjprSDtR1K4LCAtbvX0KnT7+ORHd5WRG9+1dvPi/tV8OqFdE3OaPG8x2Q7Xy3oI+0WSQ9QOA3hkr6XNJ/ArcA3YBnJc2TdFsiOZl2pOdJmk8w8jUIGAe8bGaLAcxsdXhfMfDPsB/zRmCvBHJfNLP1ZrYSWAs8Eaa/AwyWVAz0MLOXwvR70pajRqyq7MyK9V0Y3GMNAAcN+oJPIhpsqi1pT8fFG1B1HZjR+YO1VPftGInuRfM6M2BINX0GbaJdUT2jx1Ywc3o03QrZxPOd/XwH0Z8KkjoSyjI72cz6mVmRmQ00s/81s13NbJCZ7R8e4xPJyVgfqaTRwBhglJlVSpoBzAfiNduvIXCOJ4RTEWYkEL8p5nN9zHk9QZ5EMLgXCRNmHMp1Rz9PUWEdy9Z25/Jnvx2J3o1DuvL1sJ7sNGEhViA2DerM2m/tEInu+jox8dIBTLj/UwoKYfqDJXz2YTROHOCSCQvYZ/hquveo4e6nX+K+23Zh+tSBGdfr+c5OvhvTltbaFwNrQie6B3AQ0AE4XNIQM1ssqSSslRYDX4TPnZ6qYjOrkLRW0rfM7FXglFRlNsei8lJOfDDhVLOMsOq4gaw6LvP/SPGY9UJ3Zr3QPSu6//Q/+2ZFL3i+s40hauvbzhLRZ4B2khYQ1DhnAisJmvePhk3+h8J7/wT8UdJrQLpK6BfAxHCwqSpNMh3HyQHSuLIpLWSsRmpmm4Bjmrj8dKN73wB2j0m6vBm5UwiWdTWcD453zczmAPvFPHpVYqsdx8l1GkbtcwmfR+o4Tt7hYfSSRNJRwHWNkheb2QnZsMdxnNzA92xqAWY2DZiWbTscx8k9fDtmx3GcFDDIuVF7d6SO4+QXSa5aihJ3pI7j5BUNgZ1zCXekjuPkHV4jdRzHSYGGwM65hDvSGIpWbGDAda9n24zI+fDWkVnTXfJ2dgcNenycneW1ALwwJ3u685hgiajPI3Ucx0kJ7yN1HMdJBfOmveM4Tkp4H6njOE4acEfqOI6TAr7W3nEcJw3UefQnx3Gc1mM5ONiUW27dcRwnCcyU1JEISXdK+irceLMhrUTSs5I+Cv8m3M3SHanjOHlGclsxJ1lrnQIc3Sjtt8DzZrYb8Hx43izuSB3HyTvSVSM1s5eB1Y2SxwJ3h5/vBo5PJMf7SB3HyStaOI+0VNLsmPNJZjYpwTN9zKwMwMzKJCXc49wdaRoYMXod469ZTmGB8fQDJTx8S582obvH8ysofm0lAJsGdOLL03bGiqJp5Dxxwb1UbmpPnYm6+gJOnfSjSPQWFdVy02X/pqhdHYWFxstvDebuRw+IRDe03e/aVrRs87tyMxuRSXPAHWnKFBQYZ0/4gt+dtDPlZUXc/O+PmDmtmKUfddyudberqKbniytYcsW+WPsC+k3+mG6zV7FuVO+M627gzLuPo6KyU2T6AGpqCvnvCcewcVMRhYX1/PXyJ3lr/kDe/yRhpSVl2up3rTEGSTXbU+BLSf3C2mg/4KtED7SJPlJJV0m6OBOyhw6rZPmS9qxY2oHamgJmTO3BqKPWZkJVTukGoB5UUw91hqrrqC1uH53urCE2bioCoF1hPe3aGRaR5jb9XduKtA42xeNx4Ofh558DUxM94DXSFOnVt4aVy7c4kPKyIvY4oHK7113boz1rxvRl50vnUV9UQOU3iqncszgS3RDUSCae+hRm8MicPXlszp6R6S5QPbf+v8cZ0GcdU5/9Bh9EUBuFtvtdi4el6ddL0gPAaIK+1M+BK4FrgYcl/SewFPhJIjmROVJJ/wIGAR2Bv5rZJElHAxOAQoK+jCMljQRuAjoBVcAvzGxREzJPB04AOgBDgPvN7Orw2qXAacAyYCUQN/ijpHHAOICOdG5FvrZNS9dLzmXdBRtq6Tp/DYuv2Y+6zoX0n/wx3d4sZ/03SyPRf8adx1O+vgs9u1Tx91OfZEl5D97+rH8kuuutgDMvPZ4unTfx+wueZ/DANSz5POFUw5Rpq9+1eKSraW9mJzdx6ciWyImyRnqGma2W1AmYJWkqMBk4zMwWSyoJ7/sgTKuVNIbA0TY3kjAS2BuoDOU+RdCNchIwjCCPc2nCkYYjeJMAuqukxV+N8rIievev3nxe2q+GVSuKWiqmVWRTd+cP1lFT2oG6boG+9fuX0OnTryNzpOXruwCwZkMnXvxgMHsP+CoyR9rAhsoOzHu/Hwfu+3kkjrStftcaYwZ1ORbYOUprzpM0H5hJUDMdB7xsZosBzKxhLlcx8M9wpcGNwF4J5D5rZqvMrAp4FPgWcCjwmJlVmtk6gj6PjLBoXmcGDKmmz6BNtCuqZ/TYCmZOj6aJm03dtSXt6bh4A6quAzM6f7CW6r7RDDx0LKqhc/vqzZ8P2uVzPv6qJMFT6aG4WxVdOm8CoH1RLcP3Xs6y5dv/+86m7niYJXdERSQ1UkmjgTHAKDOrlDQDmA8MjXP7NcCLZnaCpMHAjATiGxeXAYqTnhHq68TESwcw4f5PKSiE6Q+W8NmH0TiUbOreOKQrXw/ryU4TFmIFYtOgzqz9VkR9hV2ruOHEaQAUFtTzzDu78sbHO0aju0cVl5z5MoUFhmS89OYQZs6LRndb/a7FI8Oj9i0mqqZ9MbAmdKJ7AAcR9GseLmlIQ9M+rJUWA1+Ez52ehOzvhN0CVQQrEM4A6oEpkq4lyONxwO3pzFAss17ozqwXumdKfM7qXnXcQFYdF/2eR1+s6c7JtyXs/88Iny4rYfxlx2dFN7Td71osRnKrlqIkKkf6DDBe0gJgEUHzfiVB8/5RSQUEc7W+A/wJuFvSRcALSch+FbgH2JVgsGk2gKSHgHnAZ8Arac2N4zhZJYvjXHGJxJGa2SbgmCYuP93o3jeA3WOSLk8g/iszOyeOzj8Af2iJnY7j5AHWdpv2juM4acPq3ZG2GElHAdc1Sl5sZicQhMFyHKcNkc05rPFo0pFKuplmuiLM7LyMWBRf1zRgWlT6HMfJXSJYa99imquRzm7mmuM4TnYwIF8cqZndHXsuqYuZbci8SY7jOM2Ta037hCubJI2S9B7wfni+n6S/Z9wyx3GcuAirT+6IimSWiN4EHAWsAjCz+cBhGbTJcRyneSzJIyKSGrU3s2XaOvxLXWbMcRzHSUCeziNdJulgwCS1B84jbOZvb6ioHe1Ks7N9Qu2KL7OiF2DPqz/Lmu5TX3ora7oB7hq6U1b1Z4t2fbPzPVd5mmZc5lsfKTAeOBsYQLAGfv/w3HEcJ0soySMaEv48mFk5cEoEtjiO4yRHvtVIJe0s6QlJKyV9JWmqpJ2jMM5xHGcbDKhXckdEJNO0vx94GOgH9Af+CTyQSaMcx3GaI9cCOyfjSGVm95hZbXjcS85VrB3HaVPk2PSnJh2ppJIwYPKLkn4rabCknSRdAjwVnYmO4ziNMCV3JIGkCyUtlPSupAcktTj0f3ODTXPYsm0HwJmx2SDYEsRxHCdylL7tmAcQTOnc08yqJD1MsHHmlJbIaW6t/ZCULHQcx8kE6W+2twM6SaoBOgPLWyMgIZL2BvYk2JMeADP7R0uVOY7jpE6LRuRLJcVGspsUbsEOgJl9IekGYCnBvm/TzWx6Sy1K6EglXQmMJnCk/ybYMuRVwB2p4zjZIfkaabmZjWjqoqSewFhgCFBBsBX8z8JB9aRJZtT+x8CRwAoz+wWwH8EOoI7jONkhfaP2Ywh221hpZjXAo8DBLTUnmaZ9lZnVS6qV1J1gt0+fkB9y/pXvMvLQlVSsbs/ZPz0kcv0jRq9j/DXLKSwwnn6ghIdviW4NdZR5f/V3vVg2oxMde9VxwpNlAMy9qZilz3dGBdCxVx2H/nEVnftkPp5ONsu8rbzvZklvYOelwEGSOhM07Y+kFUHtk6mRzpbUA5hMMJI/F8hupIkc4rkn+nPFOcOzorugwDh7whdcdsoQfjV6KEeMrWDH3TZGpj/KvO/6w6/5zh1fbZW29y/XcfwTZYydWsag0VXMm1iccTuyWeZt6X0nQpbckQgzexP4PwK/9g6BT5zU7ENxSOhIzey/zKzCzG4j2Hf+52ETP++QlPbN/hbOLWH92qJ0i02KocMqWb6kPSuWdqC2poAZU3sw6qi1kemPMu99D9xEh+Kta5vtu275T6mtUiQxKrJZ5m3pfSckjRPyzexKM9vDzPY2s1PD7eNbRHOb3x3Q3DUzm9tSZelA0mDgaYIBr4MJIlKNBYYCtxFMX/gEOMPM1kiaAbwOHAI8Dvw5eqszQ6++Naxc3n7zeXlZEXscUJlFi6Jnzo09+PhfXWjfrZ5j/pH5UITZLHN/31tI1zzSdNFcDa05h2PAt9NsS0vYDTjZzH4VTqD9EXAJcK6ZvSTp98CVwAXh/T3M7PB4giSNA8YBdCzsmnHD04ni1MBybS+bTDP8wgqGX1jBgtu78/693Rh2XmZraNksc3/fMeRLYGczOyJKQ1rIYjObF36eA+xC4CxfCtPuJgiu0sBDTQkK55RNAihuv0NefS3Ly4ro3b9683lpvxpWrciRplfE7Pz9DTx75g4Zd6TZLHN/3yERr6NPhmQGm3KR2D6MOqBHgvu3y91PF83rzIAh1fQZtIl2RfWMHlvBzOmZH3DJFdYu2VIPWPpCZ4p3rsm4zmyWeVt/31uRY0FL0j74kiXWAmskHWpmrwCnAi8leCYtXDJhAfsMX033HjXc/fRL3HfbLkyfOjAK1dTXiYmXDmDC/Z9SUAjTHyzhsw9bHG+h1USZ9xkXlbLirQ5sXFPIQ4cNYNi5a/n85Y6sXVyEBF0H1DLq6tUZ0R1LNsu8Lb3vRORTH2m+8XPgtnA+2KdAJDML/vQ/+0ahpklmvdCdWS90z4ruKPM++i/l26Tt/pOvI9MfSzbLvK2874TUZ9uArUlmiagIthrZ2cx+L2lHoK+ZZWUuqZktAfaOOb8h5vJBce4fnXmrHMeJimTniEZJMn2kfwdGASeH5+uBiRmzyHEcJxFpjEeaDpJp2n/TzA6Q9DZAODezfaKHHMdxMkaO1UiTcaQ1kgoJTZfUm5zroXAcpy2Rj037vwGPATtI+gPBiqIJGbXKcRynOfJt+pOZ3SdpDkFUFAHHm9n7GbfMcRwnHgbKsTZxMqP2OwKVwBOxaWa2NJOGOY7jNEmONe2T6SN9ii2b4HUkiCS9CNgrg3Y5juM0Sa71kSbTtN8n9jyMCnVmE7c7juO0OVq8ssnM5ko6MBPGOI7jJEW+1UglXRRzWgAcAKzMmEWO4zjNkY+DTUC3mM+1BH2mj2TGnOxiNbXUrsh8cOB4FOy7R1b0AtQu+CBruu8aulPWdAMcOC/zezw1xZzTsjfMkK13blabJkHpEZMumnWk4UT8rmb264jscRzHaRaRR4NNktqZWW1zW444juNkhXxxpAQ7hR4AzJP0OEHE+c0Bks3s0Qzb5jiOsy1pjv4U7pJ8B0FUOSPY7+2NlshIpo+0BFhFsEdTw3xSA9yROo6THdJbI/0r8IyZ/TgMyNS5pQKac6Q7hCP277LFgTaQYxVrx3HaEukatZfUHTgMOB3AzKqB6uaeiUdzjrQQ6Er83cLdkTqOkz2S90ClkmbHnE8KN7xsYGeC6Zx3SdqPYDPN882sRfu8NedIy8zs9y0R5jiOk3FaFtmp3MxGNHO9HcFY0Llm9qakvwK/BS5viUnNhdHLrY2jHcdxQhq2G0l0JMHnwOdm9mZ4/n8EjrVFNOdIj2ypMMdxnEhIUzxSM1sBLJM0NEw6EnivpeY02bQ3s8zvbes4jtMK0jwh/1zgvnDEvlU7EG9P2zFnjRGj1zH+muUUFhhPP1DCw7f0iURvaWklF//6TXr2rMJMPP3vXZg6dfdIdEP28h217sVXioqXRVEJ7P1IMFy87C9Bmoqgw0AYcnU97TK8S3Jbft9bYaR1syMzmwc014+aEHekKVJQYJw94Qt+d9LOlJcVcfO/P2LmtGKWftQx47rr6sXkyfvxyccldOpUw99uns7bb/dh6dLijOvOZr6j1l36A2OHk4zFl23pCet+kDHwPEPtYNlNouxOMeiCzE5maavvuzEi9wZwktmzKaeRtERSaRL3fZ0J/UOHVbJ8SXtWLO1AbU0BM6b2YNRRazOhahvWrO7EJx+XAFBVVcSyZd3p1asqEt3ZzHfUursNZ5vaZvHBoLAa0nVfozqCWDdt9X3HJcf2bMp7R5ptevWtYeXyLbtTl5cVUdqvJnI7duizgV12qWDRol6R6MtmvnOlzBtY+a8Cir8Vrc629L7jkcZR+7QQqSOV1EXSU5LmS3pX0omSDpT0epj2lqRukgol3SDpHUkLJJ2bQPSvw2ffkrRrqGuIpDckzZJ0TTM2jZM0W9LsGja1Ik/bplnEyxU6dqzhsste4/bbh1FZWRSJzmzmOxfKvIHlk4UKodex0RnQ1t53XHKsRhp1H+nRwHIz+x6ApGLgbeBEM5sVLteqAsYR7A01LIxAVZJA7jozGynpNOAm4PsE62dvNbN/SDq7qQfDVQ6TALqrpMVFX15WRO/+W1aUlfarYdWKaL7cAIWF9Vx2+eu8+OJOvP7awMj0ZjPf2S7zzXY8LipeEUNvr4/raDJBW3zf25CDgZ2jbtq/A4yRdJ2kQ4EdCVZQzQIws3UWRH4dA9wWfk5mKtYDMX9HhZ8PiUm/J4152IpF8zozYEg1fQZtol1RPaPHVjBzeuY7/wOMCy58i2VLu/HYo0MT355Gspnv7JZ5wNrXoGyK2O2mego7RaW1bb7vuLTlGqmZfShpOHAs8EdgOvGzqybSmxSdxOeMUF8nJl46gAn3f0pBIUx/sITPPoxmJHOvvcoZM+YzFi8u5paJ0wC4e8o+zJrVP+O6s5nvqHV/8luxfraorYB53y1gwFlG2Z2ivhoWjQ/qIl33NQZfltmvW1t93/HItcDOsgg7OiT1B1ab2UZJxxM04fdgS9O+G0HT/pcEtdKTGpr2TdVKJS0hqL1eK+lnoazjwhiqD5vZvZLOAq43s67N2dddJfZNZWdBVza3GqnP4lYj2aatbjWSrXf+pj3POludUkdI5x0G2dAfX5T4RmDerRfNSbDWPi1E3Ue6D3C9pHqgBjiLoPZ5s6ROBE50DEGQ1d2BBZJqgMnALc3I7SDpTYKuipPDtPOB+yWdz3a6x5TjtFVyrUYaddN+GjAtzqWD4qRdFB6JZA4OP17dKH0xW/pLAa5NzkrHcXKaiPs/k8FXNjmOk1eI3Bu1zxtHKukxgilRsfwmrOU6jtOW8Bpp6zCzE7Jtg+M4uYGyuhpgW/LGkTqO4wDeR+o4jpMO2vSoveM4TjrwwSbHcZxU8Rqp4zhOCkQcIi8Z3JE6jpN/uCPNXdSpIwW7Z2fNe1te755NZu1fmDXdv1iUvSnQ9xw+Mit6VZ66yxG5VyP1CPmO4+QfZskdSRAGkn9b0pOtNcdrpI7j5BfpD+x8PvA+0Op9YL1G6jhO3qH65I6EcqSBwPcIIs61Gq+ROo6TfyTfR1oqaXbM+aRwe6EGbgIuAbqlYo47Usdx8o4WDDaVNxXYWdL3ga/MbI6k0anY447UcZz8wkjXFqaHAD+QdCzQEegu6V4z+1lLBXkfqeM4eUc69rU3s9+Z2cAwOPxJwAutcaLgNVLHcfIMD+zsOI6TKi2YI5q8SJsBzGjt8+5IHcfJO3JtZZM70hQpLa3k4l+/Sc+eVZiJp/+9C1On7h6Z/hGj1zH+muUUFhhPP1DCw7f0cd3bme5Xf9eLZTM60bFXHSc8WQbA3JuKWfp8Z1QAHXvVcegfV9G5T2a3lj7/yncZeehKKla35+yfHpJRXQnJMUfqg00pUlcvJk/ejzPHHcuFF4zh+8d9xI47ro1Ed0GBcfaEL7jslCH8avRQjhhbwY67bXTd25nuXX/4Nd+546ut0vb+5TqOf6KMsVPLGDS6inkTizNqA8BzT/TninOGZ1xPMqRjsCmdtAlHKmmKpB9nQvaa1Z345OMSAKqqili2rDu9elVlQtU2DB1WyfIl7VmxtAO1NQXMmNqDUUdF48Rdd3S6+x64iQ7FW9c223fd4iVqqxSMwGSYhXNLWL+2KPOKEmFAnSV3RESbcKRRsUOfDeyySwWLFvWKRF+vvjWsXN5+83l5WRGl/Wpc93aquzFzbuzBQ4cP4JMnunDA+RVZsSFbtKkaqaQukp6SNF/Su5JOlHSgpNfDtLckdQujr9wg6R1JCySd24zMJZImSHpD0mxJB0iaJukTSePDeyTpFknvSXoK2KEZeeNCObOraze0Oq8dO9Zw2WWvcfvtw6isjOZXW3FqIVFtrui6o9fdmOEXVnDiS1+wy3EbeP/elFY45h9pjP6UDjI92HQ0sNzMvgcgqRh4GzjRzGZJ6g5UAeMI9qwfZma1kkoSyF1mZqMk3QhMIVih0BFYCNwGnAAMBfYB+gDvAXfGExSuu50EUNy5f6tKvrCwnssuf50XX9yJ118b2BoRraK8rIje/as3n5f2q2HVimicuOuOXndT7Pz9DTx75g4MOy+a7o1cINdG7TPdtH8HGCPpOkmHAjsCZWY2C8DM1plZLTAGuC38jJmtTiD38Rj5b5rZejNbCWyU1AM4DHjAzOrMbDnwQtpzthnjggvfYtnSbjz26NDMqYnDonmdGTCkmj6DNtGuqJ7RYyuYOT3zgw6uOzu6Y1m7ZEsdaOkLnSneOTvdC1nBWnBEREZrpGb2oaThwLHAH4HpxM+emkhvik3h3/qYzw3nDXmKpBj32qucMWM+Y/HiYm6ZGEQ8v3vKPsya1T/juuvrxMRLBzDh/k8pKITpD5bw2YcdM67XdUere8ZFpax4qwMb1xTy0GEDGHbuWj5/uSNrFxchQdcBtYy6OlHdI3UumbCAfYavpnuPGu5++iXuu20Xpk+NrgXWQBAhP7eqpBl1pJL6A6vN7F5JXxM04ftLOjBs2ncjaNpPB8ZLmtHQtE+iVtocLwNnSvoHQf/oEcD9KWYnLgsX9uaYo0/MhOikmPVCd2a90Op4tK47D3SP/kv5Nmm7/+TryPQ38Kf/2TdynU2hCEfkkyHTfaT7ANdLqgdqgLMIflBultSJwImOIQiqujuwQFINMBm4JQW9jwHfJmj6fwi8lIIsx3FyiYib7cmQ6ab9NCDeDl8HxUm7KDwSyRwc83kKwWDTNteAc5Kz0nGc/CLaEflk8CWijuPkHbk2ap+zjlTSYwRTomL5TVjLdRynLeM10uQwsxOybYPjODlI+ncRTZmcdaSO4zhNUu81UsdxnJRoU/NIHcdxMoI7UsdxnBQwgjWMOYQ7Usdx8gphOde093ikjuPkH/X1yR0JkDRI0ouS3pe0UNL5rTHHa6QxWNVG6hd8kG0zIqdg3z2yprstlncDd//kqKzpHv7MwqzonXdyGqJUpbdpXwv8t5nNDWN/zJH0rJm91xIh7kgdx8k70tW0N7MyoCz8vF7S+8AAghjGSeOO1HGc/CN5R1oqaXbM+aQwmPs2SBoMDAPebKk57kgdx8kzWhS0pNzMRiS6SVJX4BHgAjNb11KL3JE6jpNfGGmdRyqpiMCJ3mdmj7ZGhjtSx3HyjnQFdpYk4H+B983sL62V49OfHMfJP9K3i+ghwKnAtyXNC49jW2qO10gdx8kvjLQFLTGzVwl27UgJd6SO4+QZHiHfcRwnddyROo7jpIg70u2PEaPXMf6a5RQWGE8/UMLDt/TZ7nWXllZy8a/fpGfPKszE0//ehalTd49EN3iZR1Xmi68UFS+LohLY+5FgXeayvwRpKoIOA2HI1fW0i3JnbDOoq4tQYWLckaZIQYFx9oQv+N1JO1NeVsTN//6ImdOKWfpRx+1ad129mDx5Pz75uIROnWr4283TefvtPixdWpxx3V7m0ZV56Q+MHU4yFl+2ZYJP94OMgecZagfLbhJld4pBF0RcQ8yxGmmbmf4k6etMyB06rJLlS9qzYmkHamsKmDG1B6OOWpsJVTmle83qTnzycQkAVVVFLFvWnV69qiLR7WUeXZl3G842tc3ig0FhFazrvkb1lxk1YVsaRu2TOSKizTjSTNGrbw0rl7fffF5eVkRpvzREuMlx3bHs0GcDu+xSwaJFvSLR52UefZk3xcp/FVD8rSwoTt880rQQadNeUhfgYWAgUAhcA3wK/BXoAmwCjgQqgeuAowh+fyab2c1NyFwCPAQcESb9h5l9LGkIcD9BHp9pxqZxwDiAjnRuRZ62TYvq/WVTdwMdO9Zw2WWvcfvtw6isLIpEp5d59GUej+WThQqh17FZaGbnWNM+6j7So4HlZvY9AEnFwNvAiWY2S1J3oIrAsQ0BhplZraSSBHLXmdlISacBNwHfJ3DOt5rZPySd3dSDYSSYSQDdVdLit1NeVkTv/tWbz0v71bBqRTRf7mzqBigsrOeyy1/nxRd34vXXBkam18s8+jJvTPnjouIVMfT2+rg/LhklBwebom7avwOMkXSdpEOBHYEyM5sFYGbrzKwWGAPcFn7GzFYnkPtAzN9R4edDYtLvSWMetmLRvM4MGFJNn0GbaFdUz+ixFcycnvkBl2zrBuOCC99i2dJuPPbo0Ih0BniZR1/msax9DcqmiN1uqqewU5aMaMtNezP7UNJw4Fjgj8B0gqZ7Y9REepOik/icEerrxMRLBzDh/k8pKITpD5bw2YeZH8HNtu699ipnzJjPWLy4mFsmTgPg7in7MGtW/4zr9jKPrsw/+a1YP1vUVsC87xYw4Cyj7E5RXw2Lxgf1sK77GoMva9uj9rIIDZLUH1htZhslHU/QhN+DLU37bgRN+18S1EpPamjaN1UrDftIbzOzayX9LJR1nKTHgYfN7F5JZwHXm1nX5uzrrhL7po5MV3bzBt9qJDtks9yH/yM7W43cdfKLlC1ck1JnQHFRbzu4x4+SuveZ8tvnJBOPNFWi7iPdB7heUj1QA5xFUPu8WVInAic6BrgD2B1YIKkGmAzc0ozcDpLeJOiqODlMOx+4P9zM6pFMZMZxnCxgYJZb+zFH3bSfBkyLc+mgOGkXhUcyTDSzqxvpWsyW/lKAa5OU5ThOrhPhHNFk8JVNjuPkFzk4ap83jlTSYwRTomL5jZkNzoI5juNkkxwbbMobR2pmJ2TbBsdxcgOrb8N9pI7jOKnjgZ0dx3FSI41bjaQLd6SO4+QVBliODTZ59CfHcfILM7D65I4kkHS0pEWSPpb029aY5DVSx3HyDktT015SITAR+A7wOTBL0uNm9l5L5HiN1HGc/CN9NdKRwMdm9qmZVQMPAmNbak6ka+1zHUkrgc9a+XgpUJ5Gc/JJv+t23cmyk5n1TkW5pGdCG5KhI7Ax5nxSGDqzQdaPgaPN7Jfh+anAN83snJbY5E37GFJ5wZJmRxEcIRf1u27XHSVmdnQaxcULoNLi2qU37R3Hact8DgyKOR8ILG+pEHekjuO0ZWYBu0kaIqk9cBLweEuFeNM+fUxKfMt2q991u+68JIx3fA5BVLpC4E4za3GwVh9schzHSRFv2juO46SIO1LHcZwUcUfqOI6TIu5IM4ik0yU1t9dUw31XSbo4CpuasWGwpHezoDdjeZcU6WCqpCWSEk4Ul/R1FPYksGFKOBk9ar1Zz3smcEfq5DShg39f0mRJCyVNl9RJ0v6SZkpaIOkxST3D+2dImiDpJYINEB0n47gjDZH0L0lzwn/WcWHa0ZLmSpov6fkwbaSk1yW9Hf4dmkD0IEnPhNFlrozRd2mY9hVwHnBxuvSGNeF/SXpC0mJJ50i6KHx2pqSS8L7hoY7VwExg1zTbMDVB3p8DEpUfwG4EGxzuBVQAPwL+QbDVzL7AO8CVMff3MLPDzezPzdjXRdJTYf7elXSipAPDfM2X9JakbpIKJd0g6Z3QaZ+bwNZfh8++JWnXUNcQSW9ImiXpWqBjOvWGNeEJoY7Zkg6QNE3SJ5LGh/dI0i2S3lOwxPJY4IY023Bdgrxfk6Ds8hcz8yOYAlYS/u0EvAv0AZYBQxpd7w60Cz+PAR5pRubpQBnQK0buCGA4wT9/Z2An4GPgt2nW+zHQDegNrAXGh9duBC4IPy8ADgdKgOuBhRHnvXto58XNyBkMfBRz/hsCp7k0Jm0XYG74eQZweBLv+0fA5JjzYuBT4MDYvBJsGf5ITL5LmpG5BLg0/Hwa8GT4+XHgtPDzZKAmA3rPinm/C2Le/Vdh+g+BZwnmSv4S2AT8OOK8nw18ne3/9UwcPiF/C+dJatgXahAwDnjZgm2dMbPV4bVi4G5JuxGsyS1KIPdZM1sFIOlR4Fth+mNmVinpF6HM84AuadT7opmtB9ZLWgs8Eaa/A+wrqZig5vaSpKuAHwA7AtVR5T1MT2YVyaaYz3VAjwT3b0hC5jsENbLrgCcJarplZjYLwMzWhfaNAW4zs9owfXV8cZt5IObvjeHnQwgcN8Dfgf/MgN6GcnwH6Brz7jdK6gEcBjxgZnWSXgZqgZ9J+jLCvN8DXJdARl7iTXtA0miCGtYoM9sPeBuYT/zgBdcQOKm9geMIoss0R2MZDecWo/cB4C9p1hvrfOpjzusJahtqZMOpwCdptqHJvCd4LhFrgTWSDg3PTwVeaokAM/uQLbXjPwInNGGXmkhvUnSCz58AlRnQG/t+G7/7hgqTwea8v0AQ6SzKvG+3uCMNKAbWhDXEPYCDgA7A4ZKGACjsVwzv/SL8fHoSsr8jqURSJ+B44DXgZYIvb29gHXBM+DmdepvFzCoIHNJBwBqCWkP7NNvQZN4VDBh1I3DIreHnwPWSFgD7A79vycOS+gOVZnYvcANBvvtLOjC83k3BqP90YHz4ObYsmuLEmL9vhJ9fI1jDDfBfABnQm4iXgZPCfs/9CVoHr6TZhkR5PyXFPOQs3rQPeIbgC7MAWEQw8LKSoIn7qKQC4CuCKNp/ImjeXkTwq56IVwmaNLsC95vZbABJDwF/IOifLACOTrPeZPgFcBdBV8ZIghpmVHmfR1AjeqU5AWa2BNg75vyGmMsHxbl/dBJ2AexD4IjrgRqC/kABN4eOv4qgpn4HsDuwQFINQR9nc1PaOkh6k+CdnhymnQ/cL+l8gv7LTpLmpVlvIh4Dvk1QE64I7bueIK5oVHl/JAX7cxpfa+9kBEmnAyOshQFynfxE0hKC953N4OZZw5v2juM4KeI10jQg6Si2HY1cbGYnxLs/3/Xmmg1RI+kxYEij5N+Y2bTtUW+u2ZCLuCN1HMdJEW/aO47jpIg7UsdxnBRxR+okjaQ6SfPC9dn/lNQ5BVmbow9JukPSns3cO1rSwa3QETcaU1Ppje5pUZQi5UAELyd7uCN1WkKVme0frmyqBsbHXpRU2BqhZvZLM3uvmVtGAy12pI4TFe5IndbyCkG0qNGSXpR0P/BOuHLm+jDazwJJZ8I20YeeAnZoEKQg9N2I8PNWUackDSZw2BeGteFDJfWW9EioY5akQ8JneykIs/e2pNuJv2f5VihO1K+Ya38ObXleUu8wbRcFEa3mSHpFwUo4p43jK5ucFhMuGTyGYEUYBKui9jazxaEzWmtmB0rqALwmaTowjCBk3j4E0aXeA+5sJLc3weqZw0JZJWa2WtJtBFGDbgjvux+40cxelbQjwQ6Q3yCICvWqmf1e0vcIVmcl4oxQRydglqRHwkArXQgiSv23pCtC2ecQ7KA53sw+kvRNgiAk325FMTrbEe5InZbQsLQRghrp/xI0ud9qiBQFfJcgulRD9PVigniim6MPAcslxVtiehDxo041Zgywp7S5wtldwbr9wwjCxWFmT0lak0SeGkf92g1YRRDs46Ew/V6C5bJdw/z+M0Z3hyR0ONs57kidllBlZvvHJoQOJTZsnYBzG0/QlnQsiaMAJRttqIAgUldVHFuSnhitraN+VUqaQdMRrSzUW9G4DBzH+0iddDMNOEtSEYCk3SV1YevoQ/2AI+I8+wbxo06tJwhU3MB0gmY24X37hx9fJowwJOkYoGcCW+NF/WqgAGioVf8HQZfBOmCxpJ+EOiRpvwQ6nDaAO1In3dxB0P85V8FmercTtHweAz4iiD50K3Hih5pZbNSp+WxpWj9BEHpvnoIYpOcBI8LBrPfYMnvgauAwSXMJuhiWJrD1GaCdgqhf1xBEvmpgA7CXpDkEfaANYfpOIQjMPJ9gR4GxSZSJs53jS0Qdx3FSxGukjuM4KeKO1HEcJ0XckTqO46SIO1LHcZwUcUfqOI6TIu5IHcdxUsQdqeM4Tor8f2VqlxaVK5BkAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7402339723768295\n",
      "Recall: 0.7232142857142857\n",
      "Accuracy: 0.7232142857142857\n"
     ]
    }
   ],
   "source": [
    "# Load the test images from the folder 'test_images'\n",
    "predicted_labels= []\n",
    "true_labels = []\n",
    "directory = \"crop_dataset_test\"\n",
    "\n",
    "\n",
    "label_mapping = {\n",
    "    'aca_bd': 0,\n",
    "    'aca_md': 1,\n",
    "    'aca_pd': 2,\n",
    "    'nor' : 3,\n",
    "    'scc_bd' : 4,\n",
    "    'scc_md' : 5, \n",
    "    'scc_pd' : 6\n",
    "}\n",
    "\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "\n",
    "    # Load the image and add it to the list of test images\n",
    "    f = os.path.join(directory, filename)\n",
    "    imagen = tf.keras.preprocessing.image.load_img(f, target_size=(380,380))\n",
    "    x = tf.keras.preprocessing.image.img_to_array(imagen)    \n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    prediction = modelo.predict(x)\n",
    "    predicted_class = np.argmax(prediction)\n",
    "\n",
    "    #True labels are calculated based on name. It is a very weak way of doing it. Look for a better one or rename the files\n",
    "\n",
    "    if 'aca_bd' in filename:\n",
    "        true_labels.append(0)\n",
    "    elif 'aca_md' in filename:\n",
    "        true_labels.append(1)\n",
    "    elif 'aca_pd' in filename:\n",
    "        true_labels.append(2)\n",
    "    elif 'nor' in filename:\n",
    "        true_labels.append(3)\n",
    "    elif 'scc_bd' in filename:\n",
    "        true_labels.append(4)    \n",
    "    elif 'scc_md' in filename:\n",
    "        true_labels.append(5)\n",
    "    elif 'scc_pd' in filename:\n",
    "        true_labels.append(6)\n",
    "    \n",
    "    predicted_labels.append(predicted_class)\n",
    "\n",
    "print(predicted_labels)\n",
    "print(true_labels)\n",
    "\n",
    "\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "labels = [label for label, _ in sorted(label_mapping.items(), key=lambda x: x[1])]\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot()\n",
    "\n",
    "plt.title(directory,  fontdict={'fontsize': 16})  # Set the new title for the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Calculate precision, recall, and accuracy\n",
    "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "# Print the results\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "mlflow.log_metric(\"Precision\", precision)\n",
    "mlflow.log_metric(\"Recall\", recall)\n",
    "mlflow.log_metric(\"Accuracy\", accuracy)\n",
    "\n",
    "mlflow.end_run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fea903cc172cce62c6301ba3c7eecfb26209215bbdb6c0bb5db0de1694474dd0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
